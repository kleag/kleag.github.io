





@inproceedings{castillo_lopez_intent_2025,
	location = {Avignon, France},
	title = {Intent Recognition and Out-of-Scope Detection using {LLMsin} Multi-party Conversations},
	eventtitle = {{SIGDIAL} 2025},
	booktitle = {Proceeding of The 26th Annual Meeting of the Special Interest Group on Discourse and Dialogue ({SIGDIAL})},
	author = {Castillo López, Galo and Chalendar (de, Gaël and Semmar, Nasredine},
	date = {2025-08},
}

@inproceedings{lombard_cova_2024,
title = {CoVA: A Virtual Assistant for Virtual Reality Conferencing},
booktitle = {Proceedings of the 21st EuroXR International Conference},
author = {Lombard, Alexis and Benazzouz, Yazid and Castillo López, Galo and Chalendar (de), Gaël and Lorré, Jean\-Pierre},
date = {2024},
}

@inproceedings{oukelmoun_detection_2025,
	location = {Marseille, France},
	title = {Détection des omissions dans les résumés médicaux générés par les {LLMs}},
	eventtitle = {{CORIA}-{TALN} 2025},
	author = {Oukelmoun, Achir and Semmar, Nasredine and Chalendar (de, Gaël and Cormi, Clément and Oukelmoun, Mariame and Vibert, Eric and Allard, Marc-Antoine},
	date = {2025},
}

@inproceedings{oukelmoun_study_2023,
	title = {A Study on the Relevance of Generic Word Embeddings for Sentence Classification in Hepatic Surgery},
	url = {https://ieeexplore.ieee.org/abstract/document/10479342},
	doi = {10.1109/AICCSA59173.2023.10479342},
	abstract = {While the fine-tuning process of extensive contextual language models often demands substantial computational capacity, utilizing generic pre-trained models in highly specialized domains can yield suboptimal results. This paper aims to explore an innovative approach to derive pertinent word embeddings tailored to a specific domain with limited computational resources (The introduced methodologies are tested within the domain of hepatic surgery, utilizing the French language.). This exploration takes place within a context where computational limitations prohibit the fine-tuning of large language models. A new embedding (referred to as {FTW}2V) that combines Word2Vec and {FastText} is introduced. This approach addresses the challenge of incorporating terms absent from Word2Vec’s vocabulary. Furthermore, a novel method is used to evaluate the significance of word embeddings within a specialized corpus. This evaluation involves comparing classification scores distributions of classifiers (Gradient Boosting) trained on word embeddings derived from benchmarked Natural Language Processing ({NLP}) models. As per this assessment technique, the {FTW}2V model, trained from scratch with limited computational resources, outperforms generic contextual models in terms of word embeddings quality. Additionally, a computationally efficient contextual model rooted in {FTW}2V is introduced. This modified model substitutes Gradient Boosting with a transformer and integrates Part Of Speech labels.},
	eventtitle = {2023 20th {ACS}/{IEEE} International Conference on Computer Systems and Applications ({AICCSA})},
	pages = {1--8},
	booktitle = {2023 20th {ACS}/{IEEE} International Conference on Computer Systems and Applications ({AICCSA})},
	author = {Oukelmoun, Achir and Semmar, Nasredine and De Chalendar, Gaël and Habran, Enguerrand and Vibert, Eric and Goblet, Emma and Oukelmoun, Mariame and Allard, Marc-Antoine},
	urldate = {2025-07-06},
	date = {2023-12},
	note = {{ISSN}: 2161-5330},
	keywords = {Benchmark testing, Boosting, classifiers, Computational modeling, Gradient Boosting, hepatic, Natural language processing, Natural Language Processing, supervised learning, surgery, Surgery, transformers, Transformers, Vocabulary, Word embeddings},
}

@article{lopez_espejel_comprehensive_2023,
	title = {A comprehensive review of State-of-The-Art methods for Java code generation from Natural Language Text},
	volume = {3},
	issn = {2949-7191},
	url = {https://www.sciencedirect.com/science/article/pii/S2949719123000109},
	doi = {10.1016/j.nlp.2023.100013},
	abstract = {Java Code Generation consists in generating automatically Java code from a Natural Language Text. This {NLP} task helps in increasing programmers’ productivity by providing them with immediate solutions to the simplest and most repetitive tasks. Code generation is a challenging task because of the hard syntactic rules and the necessity of a deep understanding of the semantic aspect of the programming language. Many works tried to tackle this task using either {RNN}-based, or Transformer-based models. The latter achieved remarkable advancement in the domain and they can be divided into three groups: (1) encoder-only models, (2) decoder-only models, and (3) encoder–decoder models. In this paper, we provide a comprehensive review of the evolution and progress of deep learning models in Java code generation task. We focus on the most important methods and present their merits and limitations, as well as the objective functions used by the community. In addition, we provide a detailed description of datasets and evaluation metrics used in the literature. Finally, we discuss results of different models on {CONCODE} dataset, then propose some future directions.},
	pages = {100013},
	journaltitle = {Natural Language Processing Journal},
	shortjournal = {Natural Language Processing Journal},
	author = {López Espejel, Jessica and Yahaya Alassan, Mahaman Sanoussi and Chouham, El Mehdi and Dahhane, Walid and Ettifouri, El Hassane},
	urldate = {2025-07-06},
	date = {2023-06-01},
	keywords = {Java code generation, Language models, Natural language processing, Recurrent neural networks, Transformer neural networks},
	file = {ScienceDirect Full Text PDF:/home/gael/Zotero/storage/DSJTR7E3/López Espejel et al. - 2023 - A comprehensive review of State-of-The-Art methods for Java code generation from Natural Language Te.pdf:application/pdf;ScienceDirect Snapshot:/home/gael/Zotero/storage/Q726X9XT/S2949719123000109.html:text/html},
}

@inproceedings{lopez_espejel_automatic_2019,
	location = {Toulouse, France},
	title = {Automatic summarization of medical conversations, a review},
	url = {https://hal.science/hal-02611210},
	abstract = {Conversational analysis plays an important role in the development of simulation devices for the training of health professionals (doctors, nurses). Our goal is to develop an original automatic synthesis method for medical conversations between a patient and a healthcare professional, based on recent advances in summarization using convolutional and recurrent neural networks. The proposed method must be adapted to the specific problems related to the synthesis of dialogues. This article presents a review of the different methods for extractive and abstractive summarization, and for dialogue analysis. We also describe the use of Natural Language Processing in the medical field.},
	pages = {487--498},
	booktitle = {Actes de la Conférence sur le Traitement Automatique des Langues Naturelles ({TALN}) - {PFIA} 2019 - Volume {III} : {RECITAL}},
	publisher = {{ATALA}},
	author = {López Espejel, Jessica},
	urldate = {2025-07-06},
	date = {2019-07},
	keywords = {Automatic summarization, dialogues, medical domain, review.},
	file = {HAL PDF Full Text:/home/gael/Zotero/storage/4VIDKYBJ/Lopez - 2019 - Automatic summarization of medical conversations, a review.pdf:application/pdf},
}


@inproceedings{mnasri_resume_2015,
	location = {Caen, France},
	title = {Résumé Automatique Multi-Document Dynamique : État de l'Art},
	url = {https://aclanthology.org/2015.jeptalnrecital-recital.4/},
	shorttitle = {Résumé Automatique Multi-Document Dynamique},
	abstract = {Les travaux menés dans le cadre du résumé automatique de texte ont montré des résultats à la fois très encourageants mais qui sont toujours à améliorer. La problématique du résumé automatique ne cesse d'évoluer avec les nouveaux champs d'application qui s'imposent, ce qui augmente les contraintes liées à cette tâche. Nous nous inté- ressons au résumé extractif multi-document dynamique. Pour cela, nous examinons les différentes approches existantes en mettant l'accent sur les travaux les plus récents. Nous montrons ensuite que la performance des systèmes de résumé multi-document et dynamique est encore modeste. Trois contraintes supplémentaires sont ajoutées : la redondance inter-document, la redondance à travers le temps et la grande taille des données à traiter. Nous essayons de déceler les insuffisances des systèmes existants afin de bien définir notre problématique et guider ainsi nos prochains travaux.},
	eventtitle = {{JEP}/{TALN}/{RECITAL} 2015},
	pages = {38--49},
	booktitle = {Actes de la 22e conférence sur le Traitement Automatique des Langues Naturelles. {REncontres} jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues},
	publisher = {{ATALA}},
	author = {Mnasri, Maâli},
	editor = {Lecarpentier, Jean-Marc and Lucas, Nadine},
	urldate = {2025-07-06},
	date = {2015-06},
	file = {Full Text PDF:/home/gael/Zotero/storage/MC8L5D7M/Mnasri - 2015 - Résumé Automatique Multi-Document Dynamique  État de l'Art.pdf:application/pdf},
}


@inproceedings{mouton_induction_2009,
	location = {Senlis, France},
	title = {Induction de sens de mots à partir de multiples espaces sémantiques},
	url = {http://www-lipn.univ-paris13.fr/taln09/pdf/RECITAL_7.pdf},
	booktitle = {actes de la 11ème Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues ({RECITAL})},
	author = {Mouton, Claire},
	date = {2009-06},
}

@thesis{mouton_ressources_2010,
	title = {Ressources et méthodes semi-supervisées pour l’analyse sémantique de texte en français},
	rights = {Licence Etalab},
	url = {https://theses.fr/2010PA112375},
	abstract = {Pouvoir chercher des informations sur un niveau sémantique plutôt que purement lexical devrait améliorer la recherche d'informations. Cette thèse a pour objectif de développer des modules d'analyse sémantique lexicale afin d'améliorer le système de recherche de documents textuels de la société Exalead. Les travaux présentés concernent plus spécifiquement l'analyse sémantique de texte en français. La problématique liée au traitement du français réside dans le fait qu'il n'existe que peu de ressources sémantiques et de corpus annotés pour cette langue. Rendre possible une telle analyse implique donc d'une part de pourvoir aux besoins en ressources linguistiques françaises, et d'autre part, de trouver des méthodes alternatives ne nécessitant pas de corpus français manuellement annoté. Notre manuscrit est structuré en trois parties suivies d'une conclusion. Les deux chapitres de la première partie délimitent les objectifs et le contexte de notre travail. Le premier introduit notre thèse en évoquant la problématique de la sémantique en recherche d'information, en présentant la notion de sens et en identifiant deux tâches d'analyse sémantique : la désambiguïsation lexicale et l'analyse en rôles sémantiques. Ces deux tâches font l'objet de l'ensemble de notre étude et constituent respectivement les parties 2 et 3. Le second chapitre dresse un état de l'art de toutes les thématiques abordées dans notre travail. La deuxième partie aborde le problème de la désambiguïsation lexicale. Le chapitre 3 est consacré à la constitution de nouvelles ressources françaises pour cette tâche. Nous décrivons dans un premier temps une méthode de traduction automatique des synsets nominaux de {WordNet} vers le français à partir de dictionnaires bilingues et d'espaces distributionnels. Puis, nous constituons une ressource automatiquement en proposant une adaptation de deux méthodes d'induction de sens existantes. L'originalité des clusters de sens ainsi constitués est de contenir des mots dont la syntaxe est proche de celle des mots source. Ces clusters sont alors exploités dans l'algorithme que nous proposons au chapitre 4 pour la désambiguïsation elle-même. Le chapitre 4 fournit également des recommandations concernant l'intégration d'un tel module dans un système de recherche de documents. L'annotation en rôles sémantiques est traitée dans la troisième partie. Suivant une structure similaire, un premier chapitre traite de la constitution de ressources pour le français, tandis que le chapitre suivant présente l'algorithme développé pour l'annotation elle-même. Ainsi, le chapitre 5 décrit nos méthodes de traduction et d'enrichissement des prédicats de {FrameNet}, ainsi que l'évaluation associée. Nous proposons au chapitre 6 une méthode semi-supervisée exploitant les espaces distributionnels pour l'annotation en rôles sémantiques. Nous concluons ce chapitre par une réflexion sur l'usage des rôles sémantiques en recherche d'information et plus particulièrement dans le cadre des systèmes de réponses à des questions posées en langage naturel. La conclusion de notre mémoire résume nos contributions en soulignant le fait que chaque partie de notre travail exploite les espaces distributionnels syntaxiques et que ceci permet d'obtenir des résultats intéressants. Cette conclusion mentionne également les perspectives principales que nous inspirent ces travaux. La perspective principale et la plus immédiate est l'intégration de ces modules d'analyse sémantique dans des prototypes de recherche documentaire.},
	institution = {Paris 11},
	type = {These de doctorat},
	author = {Mouton, Claire},
	editora = {Vilnat, Anne and Chalendar, Gaël de},
	editoratype = {collaborator},
	urldate = {2025-06-26},
	date = {2010-01-01},
	keywords = {Analyse sémantique, Désambiguïsation lexicale, Espaces distributionnels, Français (langue) -- Sémantique, Induction de sens de mots, Ressources linguistiques françaises, Rôles sémantiques, Srl, {WSDL}},
}

@thesis{pradet_annotation_2015,
	title = {Annotation en rôles sémantiques du français en domaine spécifique},
	rights = {Licence Etalab},
	url = {https://theses.fr/2015USPCC091},
	abstract = {Cette thèse de Traitement Automatique des Langues a pour objectif l'annotation automatique en rôles sémantiques du français en domaine spécifique. Cette tâche désambiguïse le sens des prédicats d'un texte et annote les syntagmes liés avec des rôles sémantiques tels qu'Agent, Patient ou Destination. Elle aide de nombreuses applications dans les domaines où des corpus annotés existent, mais est difficile à utiliser quand ce n'est pas le cas. Nous avons d'abord évalué sur le corpus {FrameNet} une méthode existante d'annotation basée uniquement sur {VerbNet} et donc indépendante du domaine considéré. Nous montrons que des améliorations conséquentes peuvent être obtenues à la fois d'un point de vue syntaxique avec la prise en compte de la voix passive et d'un point de vue sémantique en utilisant les restrictions de sélection indiquées dans {VerbNet}. Pour utiliser cette méthode en français, nous traduisons deux ressources lexicales anglaises. Nous commençons par la base de données lexicales {WordNet}. Nous traduisons ensuite le lexique {VerbNet} dans lequel les verbes sont regroupés sémantiquement grâce à leurs traits syntaxiques. La traduction, {VerbNet}, a été obtenue en réutilisant deux lexiques verbaux du français (le Lexique-Grammaire et Les Verbes Français) puis en modifiant manuellement l'ensemble des informations obtenues. Enfin, une fois ces briques en place, nous évaluons la faisabilité de l'annotation en rôles sémantiques en anglais et en français dans trois domaines spécifiques. Nous évaluons quels sont les avantages et inconvénients de se baser sur {VerbNet} et {VerbuNet} pour annoter ces domaines, avant d'indiquer nos perspectives pour poursuivre ces travaux.},
	institution = {Sorbonne Paris Cité},
	type = {These de doctorat},
	author = {Pradet, Quentin},
	editora = {Danlos, Laurence and Chalendar, Gaël de},
	editoratype = {collaborator},
	urldate = {2025-06-26},
	date = {2015-01-01},
	keywords = {Analyse sémantique, {FrameNet}, Français (langue) -- Sémantique, Linguistique -- Informatique, Méthode non supervisée, Traduction automatique, Traitement automatique du langage naturel, {VerbNet}, {WordNet}, {WSDL}},
}

@thesis{lopez_espejel_resume_2021,
	title = {Résumé automatique abstractif des textes médicaux longs avec un Transformer à multi-encodeurs et évaluation de résumé du domaine général avec {wikiSERA}},
	rights = {Licence Etalab},
	url = {https://theses.fr/2021PA131010},
	abstract = {Les dernières statistiques faites par l’{IDC} (International Data Corporation)1 montrent que le volume d’information en exabytes dans le domaine médical a augmenté de plus de 1400\% entre les années 2013 et 2020. Cette croissance monstrueuse fait que des sites tel que "{PubMed}" (for Biotechnology Information, 2018) de "{MEDLINE}" (Solutions, 2021) et "Dimensions" contiennent à présent des millions d’articles médicaux portant sur des sujets variés. Cependant, et afin de suivre le rapide progrès dans le domaine médical, les chercheurs et les médecins ont besoin d’accéder aux informations pertinentes le plus rapidement possible. Grâce à l’intelligence artificielle et les avancements dans le traitement automa tique du langage naturel, le domaine du résumé automatique de textes a émergé pour le but de proposer des solutions efficaces afin de transformer un ou plusieurs textes longs en un résumé de petite taille concentrant leur information la plus utile. Les premiers travaux dans le domaine du résumé automatique étaient extractifs, où les phrases les plus pertinentes du texte sont copiées et concaténées afin de construire le résumé. Avec l’apparition de l’apprentissage profond, le résumé automatique est basé désormais sur des approches abstractives, où le système reformule le texte en un résumé qui ne contient pas forcément des mots du texte original. Malgré l’évolution dans le domaine du résumé automatique, il est nécessaire d’évaluer automatiquement la qualité des résumés générés afin de pouvoir com parer et améliorer les différentes approches de l’état de l’art. Ceci dit que le domaine d’évaluation automatique des résumés est aussi important pour le fait que l’évaluation manuelle est coûteuse en termes d’argent et de temps, même si elle constitue la meilleure référence d’évaluation. Il existe deux types d’approches automatiques d’évaluation de résumé : celles qui nécessitent une intervention humaine (telles que {ROUGE} (Lin, 2004) et {SERA} (Co han and Goharian, 2016)), et celles qui ne la nécessitent pas (telles que {SummTriver} (Cabrera Diego and Torres-Moreno, 2018) et {FRESA} (Torres-Moreno et al., 2010)). Les dernières approches ont l’avantage de fonctionner sans avoir besoin d’un résumé 1https://www.idc.com/ 196 Appendix B. Résumé en français de référence, mais elles ont jusqu’à présent une faible corrélation avec les méthodes d’évaluation manuelles. Dans cette thèse, nous nous focalisons sur le résumé automatique abstractif des textes médicaux longs, ainsi que l’évaluation automatique des résumés appartenant au domaine général. Pour la première problématique, nous proposons une amélioration de l’architecture originale des réseaux de neurones de type Transformers. Notre méthode (appelée {HazP} i) consiste à augmenter le nombre d’encodeurs du modèle en découpant l’entrée entre eux afin de concentrer l’attention du modèle sur des sous parties du texte (Multi-encoder Transformer). En plus, notre méthode favorise l’apprentissage progressif en présentant les résumés au décodeur partie par partie jusqu’à la consommation de toute la séquence (End-chunk Task Training). Nous menons des expérimentations sans et avec pré-entraînement du modèle sur des datasets médicales et les résultats obtenus sont encourageants en comparant {HazP} i avec des méthodes compétitives de l’état de l’art. Pour la deuxième problématique, nous présentons {wikiSERA}, une amélioration de la méthode {SERA} pour l’évaluation automatique des résumés biomédicaux en se basant sur l’intervention humaine. {SERA} est basée sur une analyse de la pertinence de contenu entre un résumé candidat et un ensemble de résumés de référence à l’aide d’un moteur de recherche qui compare les résultats de recherche dans un ensemble de documents qui constituent l’index, avec comme requêtes en entrée d’une part les résumés de référence et d’autre part les résumés automatiques...},
	institution = {Paris 13},
	type = {These de doctorat},
	author = {López Espejel, Jessica},
	editora = {Charnois, Thierry},
	editoratype = {collaborator},
	urldate = {2025-06-26},
	date = {2021-05-05},
	keywords = {Automatic  summarization, Automatic summary evaluation, Evaluation automatique des résumés, Exploration de données, Extraction d'informatique, Information extraction, Réseaux neuronaux (informatique), Résumé automatique, Résumé de texte -- Évaluation, Traitement automatique du langage naturel},
}


@inproceedings{chalendar_request_2002,
	location = {{LAS} {PALMAS}, {CANARY} {ISLANDS}, Spain},
	title = {Request Expansion by a Contextual Use of Classes of Nouns},
	url = {https://hal.science/hal-02456868},
	abstract = {We developed a system, {SVETLAN}', dedicated to the acquisition of classes of semantically close nouns from texts. We aim at constructing a structured lexicon for the general language, that is not for representing a specialized domain. Thus, texts are open-domain newspaper articles. The acquisition is based on a distributional method that groups the nouns that are related to a same verb with a same functional role. However, in order to deal with polysemy, classes are learned in context: they are built from text segments related to a same semantic domain. For that, we use results of {ROSA}, a system that clusters automatically segmented text thematic units in order to build semantic domain defined by sets of weighted words. We will show how these classes can be used to expand queries, in comparison with an expansion realized by using {WordNet}.},
	booktitle = {Proceeding of {LREC}},
	author = {Chalendar, Gaël De and Grau, Brigitte},
	urldate = {2025-06-25},
	date = {2002},
	file = {HAL PDF Full Text:/home/gael/Zotero/storage/DR46TLFW/Chalendar et Grau - 2002 - Request Expansion by a Contextual Use of Classes of Nouns.pdf:application/pdf},
}


@inproceedings{pagani_cortex2_2023,
	title = {{CORTEX}2 - Extended collaborative telepresence for future work and education},
	url = {https://cea.hal.science/cea-04575038},
	doi = {10.5281/zenodo.8065220},
	abstract = {In a recently launched European project {CORTEX}2, we are setting the basis for future extended collaborative telepresence to allow for remote cooperation in virtually all industrial and business sectors, both for productive work and education and training. Our idea merges the concepts of classical video-conferencing with extended reality, where real assets such as objects, machines or environment can be digitalized and shared with distant users for teamworking in a real-virtual continuous space. In essence, the {CORTEX}2 framework will allow the creation of shared working experiences between multiple distant users in different operating modes. In the Virtual Reality mode, participants will be able to create virtual meeting rooms where each user is represented by a virtual avatar. Participants will have the possibility to appear as video-based holograms in the virtual rooms, with an option to anonymise their appearance using a {AI}-based video appearance generator while keeping their original facial expressions. Participants will be able to exchange documents, 3D objects and other assets and will be accompanied by a {AI}-powered meeting assistant with extended capabilities such as natural speech interaction, meeting summarization or translation.},
	eventtitle = {{LMDE} 2023 - Leading and Managing in the Digital Rea},
	author = {Pagani, Alain and Minaskan, Narek and Javanmardi, Alireza and Xie, Yaxu and Rivier, Sylvain and Bailleau, Vincent and Helbert, Emmanuel and Noel, Pierre-Yves and Benazzouz, Yazid and Lorre, Jean-Pierre and Ohai, Franklyn and Nisevic, Maja and Vedder, Anton and Chalendar, Gaël de and Theodorou, Vasileios and Pietri, Ilia and Xezonaki, Maria-Evgenia and Andres, Florian and Szymendera, Lydia and Grzelak, Olivier and Gasch, Cristina and Garcia-Palacios, Azucena},
	urldate = {2025-06-25},
	date = {2023-06-19},
	langid = {english},
	file = {Full Text PDF:/home/gael/Zotero/storage/S79WHVFN/Pagani et al. - 2023 - CORTEX2 - Extended collaborative telepresence for future work and education.pdf:application/pdf},
}

@thesis{mnasri_resume_2018,
	title = {Résumé automatique multi-document dynamique},
	rights = {Licence Etalab},
	url = {https://theses.fr/2018SACLS342},
	abstract = {Cette thèse s’intéresse au Résumé Automatique de texte et plus particulièrement au résumémis-à-jour. Cette problématique de recherche vise à produire un résumé différentiel d'un ensemble denouveaux documents par rapport à un ensemble de documents supposés connus. Elle intègre ainsidans la problématique du résumé à la fois la question de la dimension temporelle de l'information etcelle de l’historique de l’utilisateur. Dans ce contexte, le travail présenté s'inscrit dans les approchespar extraction fondées sur une optimisation linéaire en nombres entiers ({ILP}) et s’articule autour dedeux axes principaux : la détection de la redondance des informations sélectionnées et la maximisationde leur saillance. Pour le premier axe, nous nous sommes plus particulièrement intéressés àl'exploitation des similarités inter-phrastiques pour détecter, par la définition d'une méthode deregroupement sémantique de phrases, les redondances entre les informations des nouveaux documentset celles présentes dans les documents déjà connus. Concernant notre second axe, nous avons étudiél’impact de la prise en compte de la structure discursive des documents, dans le cadre de la Théorie dela Structure Rhétorique ({RS}), pour favoriser la sélection des informations considérées comme les plusimportantes. L'intérêt des méthodes ainsi définies a été démontré dans le cadre d'évaluations menéessur les données des campagnes {TAC} et {DUC}. Enfin, l'intégration de ces critères sémantique etdiscursif au travers d'un mécanisme de fusion tardive a permis de montrer dans le même cadre lacomplémentarité de ces deux axes et le bénéfice de leur combinaison.},
	institution = {Université Paris-Saclay ({ComUE})},
	type = {These de doctorat},
	author = {Mnasri, Maâli},
	editora = {Chalendar, Gaël de},
	editoratype = {collaborator},
	urldate = {2025-07-06},
	date = {2018-09-20},
	keywords = {Analyse discursive, Analyse du discours -- Informatique, Clustering, Discourse analysis, {ILP}, Optimisation globale, Redondance (linguistique), Regroupement, Résumé de texte, Saillance linguistique, Semantic similarity, Sémantique, Similarité sémantique},
}

@inproceedings{castillo-lopez_survey_2025,
	location = {Bilbao, Spain},
	title = {A Survey of Recent Advances on Turn-taking Modeling in Spoken Dialogue Systems},
	isbn = {979-8-89176-248-0},
	url = {https://aclanthology.org/2025.iwsds-1.27/},
	abstract = {The rapid growth of dialogue systems adoption to serve humans in daily tasks has increased the realism expected from these systems. One trait of realism is the way speaking agents take their turns. We provide here a review of recent methods on turn-taking modeling and thoroughly describe the corpora used in these studies. We observe that 72\% of the reviewed works in this survey do not compare their methods with previous efforts. We argue that one of the challenges in the field is the lack of well-established benchmarks to monitor progress. This work aims to provide the community with a better understanding of the current state of research around turn-taking modeling and future directions to build more realistic spoken conversational agents.},
	pages = {254--271},
	booktitle = {Proceedings of the 15th International Workshop on Spoken Dialogue Systems Technology},
	publisher = {Association for Computational Linguistics},
	author = {Castillo-López, Galo and de Chalendar, Gael and Semmar, Nasredine},
	editor = {Torres, Maria Ines and Matsuda, Yuki and Callejas, Zoraida and del Pozo, Arantza and D'Haro, Luis Fernando},
	urldate = {2025-06-24},
	date = {2025-05},
	file = {Full Text PDF:/home/gael/Zotero/storage/E6R3NC7R/Castillo-López et al. - 2025 - A Survey of Recent Advances on Turn-taking Modeling in Spoken Dialogue Systems.pdf:application/pdf},
}

@inproceedings{de_chalendar_modelangage_2024,
	location = {Toulouse, France},
	title = {Les modèles de langage pour la génération de code tiennent-ils leurs promesses ?},
	url = {https://hal.science/hal-04678064},
	abstract = {Les modèles de langage pour la génération de code tiennent-ils leurs promesses ?},
	booktitle = {Atelier sur l'évaluation des modèles génératifs ({LLM}) et challence d'extraction d'information few-shot},
	publisher = {Institut des sciences informatiques et de leurs interactions - {CNRS} Sciences informatiques [{INS}2I-{CNRS}]},
	author = {de Chalendar, Gaël and Auda, Pauline and Deshayes-Chaussard, Jérôme and Ferret, Olivier and Hède, Patrick and Le Borgne, Hervé and Ngosso Ebene, Adolphe and Radermacher, Ansgar and Tourille, Julien},
	urldate = {2024-12-09},
	date = {2024-07},
	file = {HAL PDF Full Text:/home/gael/Zotero/storage/TSH8JLAW/de Chalendar et al. - 2024 - Les modèles de langage pour la génération de code tiennent-ils leurs promesses .pdf:application/pdf},
}

@article{bocharov_lima_2024,
	title = {From {LIMA} to {DeepLIMA}: following a new path of interoperability},
	issn = {1574-0218},
	url = {https://doi.org/10.1007/s10579-024-09773-5},
	doi = {10.1007/s10579-024-09773-5},
	shorttitle = {From {LIMA} to {DeepLIMA}},
	abstract = {In this article, we describe the architecture of the {LIMA} (Libre Multilingual Analyzer) framework and its recent evolution with the addition of new text analysis modules based on deep neural networks. We extended the functionality of {LIMA} in terms of the number of supported languages while preserving existing configurable architecture and the availability of previously developed rule-based and statistical analysis components. Models were trained for more than 60 languages on the Universal Dependencies 2.5 corpora, {WikiNer} corpora, and {CoNLL}-03 dataset. Universal Dependencies allowed us to increase the number of supported languages and generate models that could be integrated into other platforms. This integration of ubiquitous Deep Learning Natural Language Processing models and the use of standard annotated collections using Universal Dependencies can be viewed as a kind of model and data interoperability, complementary to the technical interoperability between systems.},
	journaltitle = {Language Resources and Evaluation},
	shortjournal = {Lang Resources \& Evaluation},
	author = {Bocharov, Victor and Besançon, Romaric and de Chalendar, Gaël and Ferret, Olivier and Semmar, Nasredine},
	urldate = {2024-09-23},
	date = {2024-09-20},
	langid = {english},
	keywords = {{NLP} platform, Artificial Intelligence, Interoperability, Linguistic analyzer, Neural models, Universal dependencies},
	file = {Full Text PDF:/home/gael/Zotero/storage/GE4VWTDN/Bocharov et al. - 2024 - From LIMA to DeepLIMA following a new path of interoperability.pdf:application/pdf},
}

@article{canet_toward_2011,
	title = {Toward a versatile information toolkit for end-users oriented open-sources exploitation: {VIRTUOSO}},
	journal = {Sources Ouvertes et Services”-SOS, En association avec EGC 2011},
	author = {Canet, Géraud and de Chalendar, Gael and Dubost, Laurent and Dupont, G and Dyevre, A and Khelif, K},
	year = {2011},
}

@phdthesis{de_chalendar_svetlan_2001,
	title = {{SVETLAN}', un système de structuration du lexique guidé par la détermination automatique du contexte thématique},
	school = {Université Paris Sud},
	author = {de Chalendar, Gaël},
	year = {2001},
}

@book{de_chalendar_framework_2009,
	title = {Framework modulaire de développement de ressources et d'évaluation diagnostique pour l'amélioration rapide d'un système de {TAL}},
	author = {de Chalendar, Gaël and Nouvel, Damien},
	year = {2009},
	note = {Published: Journée de l'ATALA - What French parsing systems ?},
}

@inproceedings{garcia_summarizing_2009,
	address = {Gaithersburg, Maryland, USA},
	title = {Summarizing through sense concentration and {Contextual} {Exploration} rules: the {CHORAL} system at {TAC}},
	booktitle = {Proceedings of the {Second} {Text} {Analysis} {Conference} ({TAC} 2009)},
	publisher = {National Institute of Standards and Technology},
	author = {García, Jorge and Ferret, Flores Olivier and Chalendar, Gaël De},
	month = nov,
	year = {2009},
}

@book{semmar_hybrid_2010,
	title = {A {Hybrid} {Word} {Alignment} {Approach} to {Improve} {Translation} {Lexicons} with {Compound} {Words} and {Idiomatic} {Expressions}},
	publisher = {ASLIB},
	author = {Semmar, Nasredine and Servan, Christophe and de Chalendar, Gaël and Le Ny, Benoît and Bouzaglou, Jean-Jacques},
	year = {2010},
}

@article{berthelin_trouver_2003,
	title = {Trouver des réponses sur le {Web} et dans une collection fermée},
	journal = {Workshop sur la Recherche d'information, un passage à l'échelle, Conférence INFORSID},
	author = {Berthelin, Jean-Baptiste and De Chalendar, Gaël and El Kateb, F and Ferret, Olivier and Grau, Brigitte and Hurault-Plantet, Martine and Illouz, Gabriel and Monceaux, Laura and Robba, Isabelle and Vilnat, Anne},
	year = {2003},
}

@inproceedings{de_chalendar_vers_2000,
	title = {Vers une base de connaissances structurées},
	booktitle = {Actes des {Journées} internationales d'{Orsay} sur les {Sciences} {Cognitives}, JIOSC 2000},
	author = {de Chalendar, Gaël and Grau, Brigitte},
	year = {2000},
}


@inproceedings{de_chalendar_extraction_2001,
	location = {Paris, France},
	title = {Extraction de terminologies structurées à partir de corpus généraux – une méthode et une application},
	series = {Journée d'étude de l'{ATALA}},
	booktitle = {Structuration de terminologies},
	author = {de Chalendar, Gaël and Grau, Brigitte},
	date = {2001-03},
}

@inproceedings{de_chalendar_traitement_2014,
	series = {Journée d'étude de l'{ATALA}},
	title = {Traitement {Automatique} des {Langues}, {Biens} {Communs} {Informationnels} et {Industries} de la {Langue}},
	booktitle = {Éthique et {Traitement} {Automatique} des {Langues}},
	author = {de Chalendar, Gaël},
	month = nov,
	year = {2014},
}

@article{de_chalendar_confronter_2003,
	title = {Confronter des sources de connaissances différentes pour obtenir une réponse plus fiable},
	volume = {1},
	journal = {Actes, Dixieme conférence de Traitement Automatique des Langues Naturelles (TALN 2003)},
	author = {De Chalendar, Gaël and El Kateb, F and Ferret, Olivier and Grau, Brigitte and Hurault-Plantet, Martine and Monceaux, Laura and Robba, Isabelle and Vilnat, Anne and LIMSI–Groupe, LIR},
	year = {2003},
	pages = {105--114},
}

@article{besancon_lr_2005,
	title = {L'analyseur syntaxique de lima dans la campagne d'évaluation easy},
	volume = {2},
	journal = {Actes des Ateliers de la 12e Confrence annuelle sur le Traitement Automatique des Langues Naturelles (TALN 2005)},
	author = {Besançon, Romaric and de Chalendar, Gaël},
	year = {2005},
	pages = {21},
}

@inproceedings{de_chalendar_extension_2002,
	address = {Hammamet, Tunisie},
	title = {Extension de requêtes par l'utilisation de classes de noms et dirigée par le contexte},
	booktitle = {Colloque international sur la fouille de texte},
	author = {de Chalendar, Gaël and Grau, Brigitte},
	year = {2002},
	pages = {43--66},
}

@inproceedings{de_chalendar_structuration_2002,
	title = {Structuration de domaines sémantiques},
	volume = {72},
	booktitle = {13ème journées francophones d'{Ingéniérie} des connaissances, {IC}'2002},
	author = {de Chalendar, Gaël and Grau, Brigitte},
	year = {2002},
}

@article{berthelin_getting_2003,
	title = {Getting reliable answers by exploiting results from several sources of information},
	journal = {CoLogNET-ElsNET Symposium, Question and Answers: Theoretical and Applied Perspectives, Amsterdam, Holland},
	author = {Berthelin, Jean-Baptiste and De Chalendar, Gaël and Elkateb-Gara, Faïza and Ferret, Olivier and Grau, Brigitte and Hurault-Plantet, Martine and Illouz, Gabriel and Monceaux, Laura and Robba, Isabelle and Vilnat, Anne and {others}},
	year = {2003},
}

@inproceedings{mouton_traduction_2009,
	address = {Avignon, France},
	title = {Traduction de {FrameNet} par dictionnaires bilingues avec évaluation sur la paire anglais-français},
	booktitle = {Actes de la conférence {MAJECSTIC} 2009},
	author = {Mouton, Claire and Richert, Benoit and de Chalendar, Gael},
	year = {2009},
}

@inproceedings{de_query_2002,
	address = {LAS PALMAS, CANARY ISLANDS, SPAIN},
	title = {Query {Expansion} by a {Contextual} {Use} of {Classes} of {Nouns}},
	booktitle = {{LREC} 2002 {Workshop} on {Creating} and {Using} {Semantics} for {Information} {Retrieval} and {Filtering}. {State} of the {Art} and {Future} {Research}.},
	author = {(de), Gaël Chalendar and Grau, Brigitte},
	month = may,
	year = {2002},
}

@inproceedings{delezoide_mm:_2010,
	title = {{MM}: modular architecture for multimedia information retrieval},
	booktitle = {Content-{Based} {Multimedia} {Indexing} ({CBMI}), 2010 {International} {Workshop} on},
	publisher = {IEEE},
	author = {Delezoide, Bertrand and Le Borgne, Hervé and Besançon, Romaric and De Chalendar, Gaël and Ferret, Olivier and Gara, Faıza and Hede, Patrick and Laib, Meriama and Mesnard, Olivier and Moëllic, P-A and {others}},
	year = {2010},
	pages = {1--6},
}

@inproceedings{besancon_lic2ms_2003,
	title = {The {LIC2M}'s {CLEF} 2003 {System}.},
	booktitle = {Working {Notes} for the {CLEF} 2003 {Workshop}, {Trondheim}, {Norway}},
	author = {Besançon, Romaric and de Chalendar, Gaël and Ferret, Olivier and Fluhr, Christian and Mesnard, Olivier and Naets, Hubert},
	year = {2003},
	pages = {21--22},
}

@inproceedings{de_chalendar_question_2002,
	title = {The {Question} {Answering} {System} {QALC} at {LIMSI}, {Experiments} in {Using} {Web} and {WordNet}.},
	booktitle = {{TREC}},
	author = {de Chalendar, Gaël and Dalmas, Tiphaine and Elkateb-Gara, Faiïza and Ferret, Olivier and Grau, Brigitte and Hurault-Plantet, Martine and Illouz, Gabriel and Monceaux, Laura and Robba, Isabelle and Vilnat, Anne},
	year = {2002},
}

@inproceedings{de_chalendar_svetlan_2000,
	title = {{SVETLAN} - {A} {System} to {Classify} {Words} in {Context}.},
	booktitle = {{ECAI} {Workshop} on {Ontology} {Learning}},
	author = {de Chalendar, Gaël and Grau, Brigitte},
	year = {2000},
}

@inproceedings{little_integration_2007,
	title = {Integration of {Structural} and {Semantic} {Models} for {Multimedia} {Metadata} {Management}},
	url = {http://dx.doi.org/10.1109/CBMI.2007.385390; http://dblp.uni-trier.de/rec/bib/conf/cbmi/LittleMSGUCG07},
	doi = {10.1109/CBMI.2007.385390},
	booktitle = {International {Workshop} on {Content}-{Based} {Multimedia} {Indexing}, {CBMI} '07, {Bordeaux}, {France}, {June} 25-27, 2007},
	author = {Little, Suzanne and Martinelli, Massimo and Salvetti, Ovidio and Güdükbay, Ugur and Ulusoy, Ozgür and de Chalendar, Gaël and Grefenstette, Gregory},
	year = {2007},
	pages = {40--45},
}

@inproceedings{pradet_revisiting_2013,
	address = {Poznań, Poland},
	title = {Revisiting knowledge-based semantic role labeling},
	abstract = {Semantic role labeling has seen tremendous progress in the last years, both for supervised and unsupervised approaches. The knowledge-based approaches have been neglected while they have shown to bring the best results to the related word sense disambiguation task. We contribute a simple knowledge-based approach with an easy to reproduce specification. We also present a novel approach to handle the passive voice in the context of semantic role labeling that reduces the error rate in F1 by 15.7\%, showing that significant improvements can be brought to the knowledge-based approaches while retaining key advantages of the approach: a simple approach which facilitates analysis of individual errors, does not need any hand-annotated corpora and which is not domain-specific.},
	booktitle = {Proceedings of the 6th {Language} \& {Technology} {Conference}, {LTC} 2013},
	author = {Pradet, Quentin and Chalendar (de), Gaël and Pujol, Guillaume},
	month = dec,
	year = {2013},
}

@inproceedings{dupont_evaluation_2011,
	title = {Evaluation with the {VIRTUOSO} platform: an open source platform for information extraction and retrieval evaluation},
	booktitle = {Proceedings of the 2011 workshop on {Data} {infrastructurEs} for supporting information retrieval evaluation},
	publisher = {ACM},
	author = {Dupont, Gérard M and de Chalendar, Gaël and Khelif, Khaled and Voitsekhovitch, Dmitri and Canet, Géraud and Brunessaux, Stéphan},
	year = {2011},
	pages = {13--18},
}

@inproceedings{pradet_wonef_2014,
	address = {Tartu, Estonia},
	title = {{WoNeF}, an improved, expanded and evaluated automatic {French} translation of {WordNet}.},
	booktitle = {Proceedings of the 7th {International} {Global} {Wordnet} {Conference} ({GWC} 2014)},
	author = {Pradet, Quentin and Baguenier-Desormeaux, Jeanne and Chalendar (de), Gaël and Danlos, Laurence},
	year = {2014},
}

@inproceedings{pradet_adapting_2014,
	title = {Adapting {VerbNet} to {French} using existing resources.},
	booktitle = {{LREC}},
	author = {Pradet, Quentin and Danlos, Laurence and de Chalendar, Gaël},
	year = {2014},
	pages = {1122--1126},
}

@inproceedings{candito_developing_2014,
	title = {Developing a {French} {FrameNet}: {Methodology} and {First} results.},
	booktitle = {{LREC}},
	author = {Candito, Marie and Amsili, Pascal and Barque, Lucie and Benamara, Farah and de Chalendar, Gaël and Djemaa, Marianne and Haas, Pauline and Huyghe, Richard and Mathieu, Yvette Yannick and Muller, Philippe and Sagot, Benoît and Vieu, Laure},
	year = {2014},
	pages = {1372--1379},
}

@inproceedings{besancon_concept-based_2004,
	title = {Concept-{Based} {Searching} and {Merging} for {Multilingual} {Information} {Retrieval}: {First} {Experiments} at {CLEF} 2003.},
	booktitle = {Comparative {Evaluation} of {Multilingual} {Information} {Access} {Systems}},
	publisher = {Springer},
	author = {Besançon, Romaric and de Chalendar, Gaël and Ferret, Olivier and Fluhr, Christian and Mesnard, Olivier and Naets, Hubert},
	year = {2004},
	pages = {174--184},
}

@inproceedings{de_chalendar_cost-bounded_2000,
	title = {A {Cost}-{Bounded} {Algorithm} to {Control} {Events} {Generalization}.},
	booktitle = {{ICCS}},
	author = {de Chalendar, Gaël and Grau, Brigitte and Ferret, Olivier},
	year = {2000},
	pages = {555--568},
}

@book{chalendar_de_application_2010,
	title = {Une application de veille événementielle fondée sur {WebContentC}++{Framework} et le {WebLab} {Core}},
	author = {Chalendar (de), Gaël},
	year = {2010},
	note = {Published: Conférence invité at Atelier RFIA Sources Ouvertes et Services},
}

@inproceedings{pradet_wonef_2013,
	address = {Les Sables d'Olonne, France},
	title = {{WoNeF} : amélioration, extension et évaluation d'une traduction française automatique de {WordNet}},
	booktitle = {Actes de la 20e conférence sur le {Traitement} {Automatique} des {Langues} {Naturelles} ({TALN}'2013)},
	author = {Pradet, Quentin and Baguenier-Desormeaux, Jeanne and Chalendar (de), Gaël and Danlos, Laurence},
	year = {2013},
	pages = {76--89},
}

@inproceedings{mouton_framenet_2010,
	address = {Malta},
	title = {{FrameNet} translation using bilingual dictionaries with evaluation on the {English}-{French} pair},
	booktitle = {Proceedings of {Language} {Resources} and {Evaluation} {Conference}, 2010},
	author = {Mouton, Claire and Chalendar (de), Gaël and Richert, Benoit and Vilnat, Anne},
	month = may,
	year = {2010},
}

@inproceedings{garcia-flores_syntactico-semantic_2008,
	title = {Syntactico-{Semantic} {Analysis}: {A} {Hybrid} {Sentence} {Extraction} {Strategy} for {Automatic} {Summarization}},
	doi = {10.1109/MICAI.2008.36},
	booktitle = {Artificial {Intelligence}, 2008. {MICAI} '08. {Seventh} {Mexican} {International} {Conference} on},
	author = {Garcia-Flores, Jorge and Chalendar (de), Gaël},
	month = oct,
	year = {2008},
	keywords = {hybrid sentence extraction strategy, natural language processing, reference summarization tool, syntactico-semantic analysis, text analysisautomatic summarization},
	pages = {31--36},
	file = {200800003583.pdf:/home/gael/Zotero/storage/3VAEHF92/200800003583.pdf:application/pdf},
}

@phdthesis{chalendar_de_abstraction_1997,
	type = {Mémoire de {DEA}},
	title = {Abstraction de {Schémas} à partir de {Situation} {Agrégées}},
	school = {Université Paris XI},
	author = {Chalendar (de), Gaël},
	year = {1997},
}

@inproceedings{chalendar_de_svetlan_2000,
	address = {Lausanne, Suisse},
	title = {{SVETLAN}' ou {Comment} {Classer} des {Noms} en fonction de leur {Contexte}},
	booktitle = {actes de la 7e conférence annuelle sur le {Traitement} {Automatique} des {Langues} {Naturelles}, {TALN} 2000},
	author = {Chalendar (de), Gaël and Grau, Brigitte},
	month = oct,
	year = {2000},
	pages = {81--90},
}

@inproceedings{de_chalendar_how_2000,
	title = {How to {Classify} {Words} {Using} their {Context}},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Knowledge} {Engineering} and {Knowledge} {Management}, {EKAW2000}, {Juan}-les-{Pins}, {France}, {October} 2000},
	publisher = {Springer},
	author = {de Chalendar, G. and Grau, B.},
	year = {2000},
	pages = {203--216},
}

@incollection{chalendar_de_svetlan_2000-1,
	series = {Lectures notes in artificial intelligence},
	title = {{SVETLAN}' or how to {Classify} {Words} using their {Context}},
	volume = {1937},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Knowledge} {Engineering} and {Knowledge} {Management}, {EKAW} 2000},
	publisher = {Springer},
	author = {Chalendar (de), Gaël and Grau, Brigitte},
	editor = {Dieng, Rose and Corby, Olivier},
	year = {2000},
	note = {ISBN 3-540-41119-4},
	pages = {203--216},
}

@inproceedings{chalendar_de_generalisation_2000,
	address = {Paris},
	title = {Généralisation de {Graphes} {Conceptuels}},
	booktitle = {12° {Congrès} {Francophone} {AFRIF}-{AFIA}, {RFIA}'2000},
	author = {Chalendar (de), Gaël and Grau, Brigitte and Ferret, Olivier},
	month = feb,
	year = {2000},
	pages = {359--368},
}

@inproceedings{mouton_jaws_2010,
	address = {Montréal, Canada},
	title = {{JAWS} : {Just} {Another} {WordNet} {Subset}},
	language = {english},
	booktitle = {Proc. of {TALN} 2010},
	author = {Mouton, Claire and Chalendar (de), Gaël},
	editor = {ATALA},
	month = jul,
	year = {2010},
}

@inproceedings{mnasri-etal-2017-taking,
	title = {Taking into account {Inter}-sentence {Similarity} for {Update} {Summarization}},
	url = {https://www.aclweb.org/anthology/I17-2035},
	abstract = {Following Gillick and Favre (2009), a lot of work about extractivesummarization has modeled this task by associating two contrary constraints:one aims at maximizing the coverage of the summary with respect to itsinformation content while the other represents its size limit. In this context,the notion of redundancy is only implicitly taken into account. In thisarticle, we extend the framework defined by Gillick and Favre (2009) byexamining how and to what extent integrating semantic sentence similarity intoan update summarization system can improve its results. We show more preciselythe impact of this strategy through evaluations performed on DUC 2007 and TAC2008 and 2009 datasets.},
	booktitle = {Proceedings of the {Eighth} {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 2: {Short} {Papers})},
	publisher = {Asian Federation of Natural Language Processing},
	author = {Mnasri, Maâli and de Chalendar, Gaël and Ferret, Olivier},
	month = nov,
	year = {2017},
	note = {event-place: Taipei, Taiwan},
	pages = {204--209},
	file = {Mnasri et al. - 2017 - Taking into account Inter-sentence Similarity for .pdf:/home/gael/Zotero/storage/23N2BJA4/Mnasri et al. - 2017 - Taking into account Inter-sentence Similarity for .pdf:application/pdf},
}

@inproceedings{de_chalendar_lima_2014,
	title = {The {LIMA} {Multilingual} {Analyzer} {Made} {Free}: {FLOSS} {Resources} {Adaptation} and {Correction}},
	url = {http://www.lrec-conf.org/proceedings/lrec2014/summaries/362.html},
	booktitle = {Proceedings of the {Ninth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}-2014), {Reykjavik}, {Iceland}, {May} 26-31, 2014.},
	author = {de Chalendar, Gaël},
	year = {2014},
	pages = {2932--2937},
}

@inproceedings{mnasri_integration_2016,
	address = {Paris, France},
	title = {Intégration de la similarité entre phrases comme critère pour le résumé multi-document},
	booktitle = {23ème {Conférence} sur le {Traitement} {Automatique} des {Langues} {Naturelles} ({JEP}-{TALN}-{RECITAL} 2016)},
	author = {Mnasri, Maâli and Chalendar (de), Gaël and Ferret, Olivier},
	year = {2016},
	pages = {482--489},
}

@inproceedings{mostefa_multilingual_2009,
	address = {Cairo, Egypt},
	title = {A {Multilingual} {Named} {Entities} {Corpus} for {Arabic}, {English} and {French}},
	isbn = {2-9517408-5-9},
	language = {english},
	booktitle = {Proceedings of the {Second} {International} {Conference} on {Arabic} {Language} {Resources} and {Tools}},
	publisher = {The MEDAR Consortium},
	author = {Mostefa, Djamel and Laïb, Mariama and Chaudiron, Stéphane and Choukri, Khalid and Chalendar (de), Gaël},
	editor = {Choukri, Khalid and Maegaard, Bente},
	month = apr,
	year = {2009},
}

@techreport{jacquemin_compte_2000,
	title = {Compte rendu d'activité du projet {BQR} Äpprentissage de concepts},
	institution = {Université Paris XI},
	author = {Jacquemin, Christian and Nédellec, Claire and Chalendar (de), Gaël and Faure, David and Grau, Brigitte and Paroubek, Patrick and Rouveirol, Céline and Ventos, Véronique},
	year = {2000},
	note = {URL : http://www.limsi.fr/Individu/jacquemi/BQR99/},
}

@book{chalendar_de_application_2009,
	title = {Une application de veille événementielle fondée sur la plateforme {WebContent}, {Demonstration} au colloque des 50 ans de l'{ATALA}},
	author = {Chalendar (de), Gaël and Besançon, Romaric},
	year = {2009},
	note = {Published: Demonstration},
}

@inproceedings{garcia-flores_bag_2008,
	title = {Bag of {Senses} {Versus} {Bag} of {Words}: {Comparing} {Semantic} and {Lexical} {Approaches} on {Sentence} {Extraction}},
	url = {http://www.nist.gov/tac/publications/2008/participant.papers/ceaList.proceedings.pdf},
	booktitle = {Text {Analysis} {Conference}},
	author = {Garcia-Flores, Jorge and Gillard, Laurent and Ferret, Olivier and Chalendar (de), Gaël},
	year = {2008},
}

@book{chalendar_de_plateforme_2010,
	title = {La plateforme de gestion de contenus pour le {Web} sémantique},
	author = {Chalendar (de), Gaël},
	year = {2010},
	note = {Published: Invited talk at Grand Colloque STIC 2010},
}

@inproceedings{de_chalendar_modular_2009,
	address = {Boulder, Colorado},
	title = {Modular resource development and diagnostic evaluation framework for fast {NLP} system improvement},
	url = {http://www.aclweb.org/anthology/W09-1511},
	booktitle = {Proceedings of the {Workshop} on {Software} {Engineering}, {Testing}, and {Quality} {Assurance} for {Natural} {Language} {Processing} ({SETQA}-{NLP} 2009)},
	publisher = {Association for Computational Linguistics},
	author = {de Chalendar, Gaël and Nouvel, Damien},
	month = jun,
	year = {2009},
	pages = {65--73},
}

@inproceedings{de_la_clergerie_large_2008,
	title = {Large scale production of syntactic annotations for {French}.},
	booktitle = {Proceedings of the international workshop on {Automated} {Syntactic} {Annotations} for {Interoperable} {Language} {Resources}, {Hong}-{Kong}.},
	author = {de la Clergerie, Eric V. and Ayache, Christelle and Chalendar (de), Gaël and Francopoulo, Gil and Gardent, Claire and Paroubek, Patrick},
	year = {2008},
}

@inproceedings{chalendar_de_uima_2009,
	title = {{UIMA} and {WebContent}: {Complementary} {Frameworks} for {Building} {Semantic} {Web} {Applications}},
	url = {http://2009.rmll.info/IMG/pdf/GaeldeChalendar_UIMA_LSM09_paper.pdf},
	booktitle = {Rencontres {Mondiales} du {Logicel} {Libre}},
	author = {Chalendar (de), Gaël},
	year = {2009},
}

@inproceedings{besancon_lima_2010,
	address = {Malta},
	title = {{LIMA} : {A} {Multilingual} {Framework} for {Linguistic} {Analysis} and {Linguistic} {Resources} {Development} and {Evaluation}},
	booktitle = {Proceedings of {Language} {Resources} and {Evaluation} {Conference}, 2010},
	author = {Besançon, Romaric and Chalendar (de), Gaël and Ferret, Olivier and Gara, Faïza and Semmar, Nasredine},
	month = may,
	year = {2010},
}

@inproceedings{chalendar_de_thematic_2005,
	title = {Thematic {Extraction}, {Syntactic} {Sentence} {Simplification}, and {Bilingual} {Generation}},
	url = {http://tangra.si.umich.edu/clair/intranet/bibs/summbib/Chalendar&al; ,%20 Syntactic Sentence Simplification, and Bilingual Generation.pdf},
	booktitle = {{RANLP}-{WS2005A}},
	author = {Chalendar (de), Gaël and Besançon, Romaric and Ferret, Olivier and Grefenstette, Gregory and Mesnard, Olivier},
	year = {2005},
}

@inproceedings{mouton_unsupervised_2009,
	address = {Borovets, Bulgaria},
	title = {Unsupervised {Word} {Sense} {Induction} from {Multiple} {Semantic} {Spaces} with {Locality} {Sensitive} {Hashing}},
	booktitle = {7th {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2009) - poster session},
	author = {Mouton, Claire and Pitel, Guillaume and Chalendar (de), Gaël and Vilnat, Anne},
	year = {2009},
}

@inproceedings{laleye_french_2020,
	address = {Marseille, France},
	title = {A {French} {Medical} {Conversations} {Corpus} {Annotated} for a {Virtual} {Patient} {Dialogue} {System}},
	url = {https://www.aclweb.org/anthology/2020.lrec-1.72},
	abstract = {Data-driven approaches for creating virtual patient dialogue systems require the availability of large data specific to the language,domain and clinical cases studied. Based on the lack of dialogue corpora in French for medical education, we propose an annotatedcorpus of dialogues including medical consultation interactions between doctor and patient. In this work, we detail the building processof the proposed dialogue corpus, describe the annotation guidelines and also present the statistics of its contents. We then conducted aquestion categorization task to evaluate the benefits of the proposed corpus that is made publicly available.},
	booktitle = {Proceedings of {The} 12th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Laleye, Fréjus A. A. and de Chalendar, Gaël and Blanié, Antonia and Brouquet, Antoine and Behnamou, Dan},
	month = may,
	year = {2020},
	pages = {574--580},
}

@inproceedings{laleye_semantic_2020,
	title = {Semantic similarity to improve question understanding in a virtual patient},
	url = {https://doi.org/10.1145/3341105.3373936},
	doi = {10.1145/3341105.3373936},
	booktitle = {{SAC} '20: {The} 35th {ACM}/{SIGAPP} {Symposium} on {Applied} {Computing}, online event, [{Brno}, {Czech} {Republic}], {March} 30 - {April} 3, 2020},
	publisher = {ACM},
	author = {Laleye, Fréjus A. A. and Blanié, Antonia and Brouquet, Antoine and Behnamou, Dan and Chalendar, Gaël de},
	editor = {Hung, Chih-Cheng and Cerný, Tomás and Shin, Dongwan and Bechini, Alessio},
	year = {2020},
	pages = {859--866},
}

@inproceedings{laleye_iagotchi_2020,
	address = {Nancy, France},
	title = {Iagotchi : vers un agent conversationnel artistique},
	shorttitle = {Iagotchi},
	url = {https://hal.archives-ouvertes.fr/hal-02768509},
	abstract = {Cet article décrit Iagotchi, un personnage virtuel philosophique et artistique qui apprend et développe des connaissances à partir de ses interactions avec l’humain. Iagotchi se présente à la fois comme un apprenant et un expert avec comme objectifs principaux (1) d’accompagner l’homme dans ses questionnements, (2) de lui fournir des réponses pertinentes sur la base de ses requêtes et (3) de générer des textes poétiques cohérents. Dans ce travail, nous décrivons l’architecture du système de Iagotchi et les composants clés tels que le moteur de conversation, le gestionnaire de sujets et le générateur de poésies.},
	urldate = {2021-09-27},
	booktitle = {6e conférence conjointe {Journées} d'Études sur la {Parole} ({JEP}, 33e édition), {Traitement} {Automatique} des {Langues} {Naturelles} ({TALN}, 27e édition), {Rencontre} des Étudiants {Chercheurs} en {Informatique} pour le {Traitement} {Automatique} des {Langues} ({RÉCITAL}, 22e édition)},
	publisher = {ATALA},
	author = {Laleye, Frejus and de Chalendar, Gaël and Frey, Léopold and Berenguer, Rocio},
	editor = {Benzitoun, Christophe and Braud, Chloé and Huber, Laurine and Langlois, David and Ouni, Slim and Pogodalla, Sylvain and Schneider, Stéphane},
	year = {2020},
	keywords = {Agents conversationnels, Art et Science., Chatbots, Iagotchi},
	pages = {42--45},
	file = {HAL PDF Full Text:/home/gael/Zotero/storage/V4YZFKVF/Laleye et al. - 2020 - Iagotchi  vers un agent conversationnel artistiqu.pdf:application/pdf},
}


@inproceedings{lopez_espejel_saucissonnage_2021,
	location = {Montpellier, France},
	title = {Saucissonnage of Long Sequences into a Multi-encoder for Neural Text Summarization with Transformers},
	url = {https://hal.science/hal-04090684},
	abstract = {Transformer deep models have gained lots of attraction in Neural Text Summarization. The problem with existing Transformer-based systems is that they truncate documents considerably before feeding them to the network. In this paper, we are particularly interested in biomedical long text summarization. However, current input sequences are far shorter than the average length of biomedical articles. To handle this problem, we propose two improvements to the original Transformer model that allow a faster training of long sequences without penalizing the summary quality. First, we split the input between four encoders to focus attention on smaller segments of the input. Second, we use end-chunk task training at the decoder level for progressive fast decoding. We evaluate our proposed architecture on {PubMed}, a well-known biomedical dataset. The comparison with competitive baselines shows that our approach: (1) allows reading large input sequences, (2) reduces the training time considerably, and (3) slightly improves the quality of generated summaries.},
	booktitle = {Extraction et Gestion des Connaissances ({EGC}) 2021},
	author = {López Espejel, Jessica and de Chalendar, Gaël and Flores, Jorge Garcia and Meza Ruiz, Ivan Vladimir and Charnois, Thierry},
	urldate = {2025-06-25},
	date = {2021-01},
	keywords = {automatic sunmmarization, natural language processing, transformers},
	file = {HAL PDF Full Text:/home/gael/Zotero/storage/RUMJ5RA2/López Espejel et al. - 2021 - Saucissonnage of Long Sequences into a Multi-encoder for Neural Text Summarization with Transformers.pdf:application/pdf},
}


@inproceedings{bocharov_russian_2020,
	address = {Moscow},
	title = {The {Russian} {Language} {Pipeline} in the {LIMA} {Multilingual} {Analyzer}},
	url = {https://www.dialog-21.ru/en/digest/2020/articles/},
	doi = {10.28995/2075-7182-2020-19-93-105},
	abstract = {In this paper we describe the implementation of Russian language pipeline in LIMA multilingual analyzer and the results obtained in GramEval-2020 shared task. LIMA is a modular pipeline that implements rule-based and machine learning analysis components. Russian language pipeline includes deep neural networks based modules for tokenization, sentence segmentation, part of speech tagging, lemmatization and dependency parsing. Part of speech tags, feature tags and dependency trees conform to Universal Dependencies rules.},
	booktitle = {Computational {Linguistics} and {Intellectual} {Technologies}: {Proceedings} of the {International} {Conference} “{Dialogue} 2020”},
	author = {Bocharov, Victor and de Chalendar, Gaël},
	month = jun,
	year = {2020},
	file = {Full Text PDF:/home/gael/Zotero/storage/KBUY3NPD/Bocharov and de Chalendar - 2020 - THE RUSSIAN LANGUAGE PIPELINE IN THE LIMA MULTILIN.pdf:application/pdf},
}

@inproceedings{lopez_espejel_gesera_2021,
	address = {Held Online},
	title = {{GeSERA}: {General}-domain {Summary} {Evaluation} by {Relevance} {Analysis}},
	shorttitle = {{GeSERA}},
	url = {https://aclanthology.org/2021.ranlp-main.98},
	abstract = {We present GeSERA, an open-source improved version of SERA for evaluating automatic extractive and abstractive summaries from the general domain. SERA is based on a search engine that compares candidate and reference summaries (called queries) against an information retrieval document base (called index). SERA was originally designed for the biomedical domain only, where it showed a better correlation with manual methods than the widely used lexical-based ROUGE method. In this paper, we take out SERA from the biomedical domain to the general one by adapting its content-based method to successfully evaluate summaries from the general domain. First, we improve the query reformulation strategy with POS Tags analysis of general-domain corpora. Second, we replace the biomedical index used in SERA with two article collections from AQUAINT-2 and Wikipedia. We conduct experiments with TAC2008, TAC2009, and CNNDM datasets. Results show that, in most cases, GeSERA achieves higher correlations with manual evaluation methods than SERA, while it reduces its gap with ROUGE for general-domain summary evaluation. GeSERA even surpasses ROUGE in two cases of TAC2009. Finally, we conduct extensive experiments and provide a comprehensive study of the impact of human annotators and the index size on summary evaluation with SERA and GeSERA.},
	urldate = {2022-02-16},
	booktitle = {Proceedings of the {International} {Conference} on {Recent} {Advances} in {Natural} {Language} {Processing} ({RANLP} 2021)},
	publisher = {INCOMA Ltd.},
	author = {López Espejel, Jessica and de Chalendar, Gaël and Garcia Flores, Jorge and Charnois, Thierry and Meza Ruiz, Ivan Vladimir},
	month = sep,
	year = {2021},
	keywords = {summarization},
	pages = {856--867},
	file = {Full Text PDF:/home/gael/Zotero/storage/JGGBI7K5/López Espejel et al. - 2021 - GeSERA General-domain Summary Evaluation by Relev.pdf:application/pdf},
}

@inproceedings{puccetti_combining_2021,
	address = {New York, NY, USA},
	series = {{FTfJP} 2021},
	title = {Combining formal and machine learning techniques for the generation of {JML} specifications},
	isbn = {978-1-4503-8543-5},
	url = {https://doi.org/10.1145/3464971.3468425},
	doi = {10.1145/3464971.3468425},
	abstract = {Producing maintainable programs is a big challenge for the software industry as it requires solid Engineering skills and efficient CASE tools. Often, industrial programs are of a very large size (more than 1M SLOC), use high-level programming languages to their full extent (e.g. C++20, Ada 2005 or Java 16), are provided with scarce and often outdated documentation partially written in natural language. Maintenance engineers are therefore in need to understand the application at hand starting from the material left behind by the developers. The European H2020 Project DECODER (https://www.decoder- project.eu) addresses this problem by proposing to combine Natural Language Processing techniques and Formal Methods to turn as best as possible code artifacts into formal data allowing to reduce the maintenance costs and thus the total costs of ownership. In this context, we will show how to generate JML annotations using a combination of 1) automatic generation of minimal predicates, 2) Natural Language Processing (NLP) based predicates generator, and 3) manual refinement and correction, to instrument and enhance code and documentation. We will illustrate it on code samples from the MyThaiStar (https://github.com/devonfw/my- thai-star) application developed with the CASE tool devonfw by CAP GEMINI, and the Joram JMS implementation (https://gitlab.ow2.org/joram/joram) from OW2 code base.},
	urldate = {2022-11-15},
	booktitle = {Proceedings of the 23rd {ACM} {International} {Workshop} on {Formal} {Techniques} for {Java}-like {Programs}},
	publisher = {Association for Computing Machinery},
	author = {Puccetti, Armand and de Chalendar, Gaël and Gibello, Pierre-Yves},
	month = jul,
	year = {2021},
	keywords = {Machine Learning, JML, NER, OpenJML, SRL},
	pages = {59--64},
}

@article{robledo_open-source_2022,
	title = {An open-source natural language processing toolkit to support software development: addressing automatic bug detection, code summarisation and code search [version 1; peer review: 2 approved with reservations]},
	volume = {2},
	doi = {10.12688/openreseurope.14507.1},
	number = {37},
	journal = {Open Research Europe},
	author = {Robledo, C and Sallicati, F and de Chalendar, G and Fernandez, M and de Castro, P and Martin, E and Gutierrez, J and Bouachera, Y},
	year = {2022},
}

@inproceedings{moradshahi_x-risawoz_2023,
	address = {Toronto, Canada},
	title = {X-{RiSAWOZ}: {High}-{Quality} {End}-to-{End} {Multilingual} {Dialogue} {Datasets} and {Few}-shot {Agents}},
	shorttitle = {X-{RiSAWOZ}},
	url = {https://aclanthology.org/2023.findings-acl.174},
	doi = {10.18653/v1/2023.findings-acl.174},
	abstract = {Task-oriented dialogue research has mainly focused on a few popular languages like English and Chinese, due to the high dataset creation cost for a new language. To reduce the cost, we apply manual editing to automatically translated data. We create a new multilingual benchmark, X-RiSAWOZ, by translating the Chinese RiSAWOZ to 4 languages: English, French, Hindi, Korean; and a code-mixed English-Hindi language.X-RiSAWOZ has more than 18,000 human-verified dialogue utterances for each language, and unlike most multilingual prior work, is an end-to-end dataset for building fully-functioning agents. The many difficulties we encountered in creating X-RiSAWOZ led us to develop a toolset to accelerate the post-editing of a new language dataset after translation. This toolset improves machine translation with a hybrid entity alignment technique that combines neural with dictionary-based methods, along with many automated and semi-automated validation checks. We establish strong baselines for X-RiSAWOZ by training dialogue agents in the zero- and few-shot settings where limited gold data is available in the target language. Our results suggest that our translation and post-editing methodology and toolset can be used to create new high-quality multilingual dialogue agents cost-effectively. Our dataset, code, and toolkit are released open-source.},
	urldate = {2024-03-19},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Moradshahi, Mehrad and Shen, Tianhao and Bali, Kalika and Choudhury, Monojit and de Chalendar, Gael and Goel, Anmol and Kim, Sungkyun and Kodali, Prashant and Kumaraguru, Ponnurangam and Semmar, Nasredine and Semnani, Sina and Seo, Jiwon and Seshadri, Vivek and Shrivastava, Manish and Sun, Michael and Yadavalli, Aditya and You, Chaobin and Xiong, Deyi and Lam, Monica},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {2773--2794},
	file = {Full Text PDF:/home/gael/Zotero/storage/Z3DE6EB3/Moradshahi et al. - 2023 - X-RiSAWOZ High-Quality End-to-End Multilingual Di.pdf:application/pdf},
}

@inproceedings{laleye_hybridation_2019,
	address = {Toulouse, France},
	title = {Hybridation d'un agent conversationnel avec des plongements lexicaux pour la formation au diagnostic médical ({Hybridization} of a conversational agent with word embeddings for medical diagnostic training)},
	url = {https://aclanthology.org/2019.jeptalnrecital-court.17},
	abstract = {Dans le contexte médical, un patient ou médecin virtuel dialoguant permet de former les apprenants au diagnostic médical via la simulation de manière autonome. Dans ce travail, nous avons exploité les propriétés sémantiques capturées par les représentations distribuées de mots pour la recherche de questions similaires dans le système de dialogues d'un agent conversationnel médical. Deux systèmes de dialogues ont été créés et évalués sur des jeux de données collectées lors des tests avec les apprenants. Le premier système fondé sur la correspondance de règles de dialogue créées à la main présente une performance globale de 92\% comme taux de réponses cohérentes sur le cas clinique étudié tandis que le second système qui combine les règles de dialogue et la similarité sémantique réalise une performance de 97\% de réponses cohérentes en réduisant de 7\% les erreurs de compréhension par rapport au système de correspondance de règles.},
	language = {French},
	urldate = {2024-03-19},
	booktitle = {Actes de la {Conférence} sur le {Traitement} {Automatique} des {Langues} {Naturelles} ({TALN}) {PFIA} 2019. {Volume} {II} : {Articles} courts},
	publisher = {ATALA},
	author = {Laleye, Fréjus A. A. and de Chalendar, Gaël and Brouquet, Antoine and Blanié, Antonia and Benhamou, Dan},
	editor = {Morin, Emmanuel and Rosset, Sophie and Zweigenbaum, Pierre},
	month = jul,
	year = {2019},
	pages = {313--322},
	file = {Full Text PDF:/home/gael/Zotero/storage/GSSQLCYB/Laleye et al. - 2019 - Hybridation d'un agent conversationnel avec des pl.pdf:application/pdf},
}

@inproceedings{puccetti_combining_2021-1,
	address = {New York, NY, USA},
	series = {{FTfJP} '21},
	title = {Combining formal and machine learning techniques for the generation of {JML} specifications},
	isbn = {978-1-4503-8543-5},
	url = {https://doi.org/10.1145/3464971.3468425},
	doi = {10.1145/3464971.3468425},
	abstract = {Producing maintainable programs is a big challenge for the software industry as it requires solid Engineering skills and efficient CASE tools. Often, industrial programs are of a very large size (more than 1M SLOC), use high-level programming languages to their full extent (e.g. C++20, Ada 2005 or Java 16), are provided with scarce and often outdated documentation partially written in natural language. Maintenance engineers are therefore in need to understand the application at hand starting from the material left behind by the developers. The European H2020 Project DECODER (https://www.decoder- project.eu) addresses this problem by proposing to combine Natural Language Processing techniques and Formal Methods to turn as best as possible code artifacts into formal data allowing to reduce the maintenance costs and thus the total costs of ownership. In this context, we will show how to generate JML annotations using a combination of 1) automatic generation of minimal predicates, 2) Natural Language Processing (NLP) based predicates generator, and 3) manual refinement and correction, to instrument and enhance code and documentation. We will illustrate it on code samples from the MyThaiStar (https://github.com/devonfw/my- thai-star) application developed with the CASE tool devonfw by CAP GEMINI, and the Joram JMS implementation (https://gitlab.ow2.org/joram/joram) from OW2 code base.},
	urldate = {2024-03-19},
	booktitle = {Proceedings of the 23rd {ACM} {International} {Workshop} on {Formal} {Techniques} for {Java}-like {Programs}},
	publisher = {Association for Computing Machinery},
	author = {Puccetti, Armand and de Chalendar, Gaël and Gibello, Pierre-Yves},
	month = jul,
	year = {2021},
	keywords = {Machine Learning, JML, NER, OpenJML, SRL},
	pages = {59--64},
}


@inproceedings{chalendar_abstraction_1998,
	location = {Collège de France, Paris},
	title = {Abstraction de Schémas à partir de Textes à l'aide d'un Système de Généralisation de Graphes Conceptuels},
	booktitle = {Assises du Réseau I.d.F. des Sc.Co.},
	author = {Chalendar, Gaël de and Grau, Brigitte and Ferret, Olivier},
	date = {1998-10},
	year = {1998},
}

